{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook to create a regional rainfall-runoff model using an LSTM network for CAMELS CH Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Description**\n",
    "\n",
    "The following notebook contains the code to create, train, validate and test a rainfall-runoff model using a LSTM \n",
    "network architecture. The code allows for the creation of single-basin models, but it is conceptualized to create \n",
    "regional models. The code is intended as an intial introduction to the topic, in which we prioritized interpretability\n",
    "over modularity. The model generated in this code makes use of the CAMELS CH Datset [2]\n",
    "\n",
    "The logic of the code is heavily based on [Neural Hydrology](https://doi.org/10.21105/joss.04050)[1]. For a more \n",
    "flexible, robust and modular implementation of deep learning method in hydrological modeling we advice the use of Neural \n",
    "Hydrology. \n",
    "\n",
    "**Authors:**\n",
    "- Eduardo Acuna Espinoza (eduardo.espinoza@kit.edu)\n",
    "- Ralf Loritz\n",
    "- Manuel Álvarez Chaves\n",
    "\n",
    "**Adaptations:**\n",
    "- Sanika Baste (sanika.baste@kit.edu)\n",
    "\n",
    "**References:**\n",
    "\n",
    "[1]: \"F. Kratzert, M. Gauch, G. Nearing and D. Klotz: NeuralHydrology -- A Python library for Deep Learning research in hydrology. Journal of Open Source Software, 7, 4050, doi: 10.21105/joss.04050, 2022\"\n",
    "\n",
    "[2]: Höge, M., Kauzlaric, M., Siber, R., Schönenberger, U., Horton, P., Schwanbeck, J., Floriancic, M. G., Viviroli, D., Wilhelm, S., Sikorska-Senoner, A. E., Addor, N., Brunner, M., Pool, S., Zappa, M., and Fenicia, F.: CAMELS-CH: hydro-meteorological time series and landscape attributes for 331 catchments in hydrologic Switzerland, Earth Syst. Sci. Data, 15, 5755–5784, https://doi.org/10.5194/essd-15-5755-2023, 2023."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary packages\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "sys.path.append(\"../aux_functions\")\n",
    "sys.path.append(\"../datasetzoo\")\n",
    "sys.path.append(\"../modelzoo\")\n",
    "\n",
    "# Import classes and functions from other files\n",
    "from functions_training import nse_basin_averaged\n",
    "from functions_evaluation import nse\n",
    "from functions_aux import create_folder, set_random_seed, write_report\n",
    "\n",
    "# Import dataset to use\n",
    "from camelsch import CAMELS_CH\n",
    "\n",
    "# Import model\n",
    "from cudalstm import CudaLSTM\n",
    "from customlstm import customLSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1. Initialize information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define experiment nae\n",
    "experiment_name = \"LSTM_CAMELS_CH\"\n",
    "\n",
    "# paths to access the information\n",
    "path_entities = \"../../data/basin_id/basins_camels_ch_331.txt\"\n",
    "path_data = \"../../data/CAMELS_CH\"\n",
    "\n",
    "# dynamic forcings and target\n",
    "dynamic_input = ['precipitation(mm/d)', 'temperature_min(degC)', 'temperature_max(degC)', 'rel_sun_dur(%)']\n",
    "target = ['discharge_spec(mm/d)']\n",
    "\n",
    "# static attributes that will be used\n",
    "static_input = ['area',\n",
    "                'elev_mean',\n",
    "                'slope_mean',\n",
    "                'sand_perc',\n",
    "                'silt_perc',\n",
    "                'clay_perc',\n",
    "                'porosity',\n",
    "                'conductivity',\n",
    "                'glac_area',\n",
    "                'dwood_perc',\n",
    "                'ewood_perc',\n",
    "                'crop_perc',\n",
    "                'urban_perc',\n",
    "                'reservoir_cap',\n",
    "                'p_mean',\n",
    "                'pet_mean',\n",
    "                'p_seasonality',\n",
    "                'frac_snow',\n",
    "                'high_prec_freq',\n",
    "                'low_prec_freq',\n",
    "                'high_prec_dur',\n",
    "                'low_prec_dur']\n",
    "\n",
    "# time periods\n",
    "training_period = ['1995-10-01','2005-09-30']\n",
    "validation_period = ['2005-10-01','2010-09-30']\n",
    "testing_period = ['2010-10-01','2015-09-30']\n",
    "\n",
    "model_hyper_parameters = {\n",
    "    \"input_size_lstm\": len(dynamic_input) + len(static_input),\n",
    "    \"no_of_layers\":1,  \n",
    "    \"seq_length\": 365,\n",
    "    \"hidden_size\": 64,\n",
    "    \"batch_size_training\":256,\n",
    "    \"batch_size_evaluation\":1826,\n",
    "    \"no_of_epochs\": 10,             \n",
    "    \"drop_out_rate\": 0.4, \n",
    "    \"learning_rate\": 0.001,\n",
    "    \"adapt_learning_rate_epoch\": 10,\n",
    "    \"adapt_gamma_learning_rate\": 0.5,\n",
    "    \"set_forget_gate\":3,\n",
    "    \"validate_every\": 10,\n",
    "    \"validate_n_random_basins\": 5\n",
    "    }\n",
    "\n",
    "# device to train the model\n",
    "running_device = \"cpu\" #cpu or gpu\n",
    "\n",
    "# define random seed\n",
    "seed = 42\n",
    "\n",
    "# colorblind friendly palette for plotting\n",
    "color_palette = {\"observed\": \"#1f78b4\",\"simulated\": \"#ff7f00\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder '../results/LSTM_CAMELS_CH' already exists.\n"
     ]
    }
   ],
   "source": [
    "# Create folder to store the results\n",
    "path_save_folder = \"../results/\"+experiment_name\n",
    "create_folder(folder_path=path_save_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if model will be run in gpu or cpu and define device\n",
    "if running_device == \"gpu\":\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    device= f'cuda:0'\n",
    "elif running_device == \"cpu\":\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2. Create dataset and dataloader for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset training\n",
    "training_dataset = CAMELS_CH(dynamic_input= dynamic_input,\n",
    "                             target= target, \n",
    "                             sequence_length= model_hyper_parameters[\"seq_length\"],\n",
    "                             time_period= training_period,\n",
    "                             path_data= path_data,\n",
    "                             path_entities= path_entities,\n",
    "                             static_input= static_input,\n",
    "                             check_NaN= True)\n",
    "\n",
    "training_dataset.calculate_basin_std()\n",
    "training_dataset.calculate_global_statistics(path_save_scaler=path_save_folder)\n",
    "training_dataset.standardize_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader training\n",
    "train_loader = DataLoader(dataset = training_dataset, \n",
    "                          batch_size = model_hyper_parameters[\"batch_size_training\"],\n",
    "                          shuffle = True,\n",
    "                          drop_last = True)\n",
    "\n",
    "print(\"Batches in training: \", len(train_loader))\n",
    "sample = next(iter(train_loader))\n",
    "print(f'x_lstm: {sample[\"x_lstm\"].shape} | y_obs: {sample[\"y_obs\"].shape} | basin_std: {sample[\"basin_std\"].shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 3. Create dataset for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will create an individual dataset per basin. This will give us more flexibility\n",
    "entities_ids = np.loadtxt(path_entities, dtype=\"str\").tolist()\n",
    "validation_dataset = {}\n",
    "\n",
    "for entity in entities_ids:\n",
    "    dataset = CAMELS_CH(dynamic_input= dynamic_input,\n",
    "                        target= target, \n",
    "                        sequence_length= model_hyper_parameters[\"seq_length\"],\n",
    "                        time_period= validation_period,\n",
    "                        path_data= path_data,\n",
    "                        entity= entity,\n",
    "                        static_input= static_input,\n",
    "                        check_NaN= False)\n",
    "    \n",
    "    dataset.scaler = training_dataset.scaler\n",
    "    dataset.standardize_data(standardize_output=False)\n",
    "    validation_dataset[entity]= dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 4. Train LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct model\n",
    "set_random_seed(seed=seed)\n",
    "lstm_model = CudaLSTM(hyperparameters=model_hyper_parameters).to(device)\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.Adam(lstm_model.parameters(),\n",
    "                             lr=model_hyper_parameters[\"learning_rate\"])\n",
    "    \n",
    "# define learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                            step_size=model_hyper_parameters[\"adapt_learning_rate_epoch\"],\n",
    "                                            gamma=model_hyper_parameters[\"adapt_gamma_learning_rate\"])\n",
    "\n",
    "# set forget gate to 3 to ensure that the model is capable to learn long term dependencies\n",
    "lstm_model.lstm.bias_hh_l0.data[model_hyper_parameters[\"hidden_size\"]:2 * model_hyper_parameters[\"hidden_size\"]]=\\\n",
    "    model_hyper_parameters[\"set_forget_gate\"]\n",
    "\n",
    "training_time = time.time()\n",
    "# Loop through the different epochs\n",
    "for epoch in range(1, model_hyper_parameters[\"no_of_epochs\"]+1):\n",
    "    \n",
    "    epoch_start_time = time.time()\n",
    "    total_loss = []\n",
    "    # Training -------------------------------------------------------------------------------------------------------\n",
    "    lstm_model.train()\n",
    "    for sample in train_loader: \n",
    "        optimizer.zero_grad() # sets gradients of weigths and bias to zero\n",
    "        pred  = lstm_model(sample[\"x_lstm\"].to(device)) # forward call\n",
    "        \n",
    "        loss = nse_basin_averaged(y_sim=pred[\"y_hat\"], \n",
    "                                  y_obs=sample[\"y_obs\"].to(device), \n",
    "                                  per_basin_target_std=sample[\"basin_std\"].to(device))\n",
    "        \n",
    "        loss.backward() # backpropagates\n",
    "        optimizer.step() #update weights\n",
    "        total_loss.append(loss.item())\n",
    "        \n",
    "        # remove from cuda\n",
    "        del sample[\"x_lstm\"], sample[\"y_obs\"], sample[\"basin_std\"], pred\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    #training report  \n",
    "    report = f'Epoch: {epoch:<2} | Loss training: {\"%.3f \"% (np.mean(total_loss))}'\n",
    "    \n",
    "    # Validation -----------------------------------------------------------------------------------------------------\n",
    "    if epoch % model_hyper_parameters[\"validate_every\"] == 0:\n",
    "        lstm_model.eval()\n",
    "        validation_results = {}\n",
    "        with torch.no_grad():\n",
    "            # If we define validate_n_random_basins as 0 or negative, we take all the basins\n",
    "            if model_hyper_parameters[\"validate_n_random_basins\"] <= 0:\n",
    "                validation_basin_ids = validation_dataset.keys()\n",
    "            else:\n",
    "                keys = list(validation_dataset.keys())\n",
    "                validation_basin_ids = random.sample(keys, model_hyper_parameters[\"validate_n_random_basins\"])\n",
    "            \n",
    "            # go through each basin that will be used for validation\n",
    "            for basin in validation_basin_ids:\n",
    "                loader = DataLoader(dataset=validation_dataset[basin], \n",
    "                                    batch_size=model_hyper_parameters[\"batch_size_evaluation\"], \n",
    "                                    shuffle=False, \n",
    "                                    drop_last = False)\n",
    "                \n",
    "                df_ts = pd.DataFrame()\n",
    "                for sample in loader:\n",
    "                    pred  = lstm_model(sample[\"x_lstm\"].to(device)) \n",
    "                    # backtransformed information\n",
    "                    y_sim = pred[\"y_hat\"]* validation_dataset[basin].scaler[\"y_std\"].to(device) +\\\n",
    "                        validation_dataset[basin].scaler[\"y_mean\"].to(device)\n",
    "\n",
    "                    # join results in a dataframe and store them in a dictionary (is easier to plot later)\n",
    "                    df = pd.DataFrame({\"y_obs\": sample[\"y_obs\"].flatten().cpu().detach(), \n",
    "                                       \"y_sim\": y_sim.flatten().cpu().detach()}, \n",
    "                                      index=pd.to_datetime(sample[\"date\"]))\n",
    "\n",
    "                    df_ts = pd.concat([df_ts, df], axis=0)\n",
    "\n",
    "                    # remove from cuda\n",
    "                    del pred, y_sim\n",
    "                    torch.cuda.empty_cache()       \n",
    "                \n",
    "                validation_results[basin] = df_ts\n",
    "                 \n",
    "            #average loss validation\n",
    "            loss_validation = nse(df_results=validation_results)\n",
    "            report += f'| Loss validation: {\"%.3f \"% (loss_validation)}'\n",
    "\n",
    "    \n",
    "    # save model after every epoch\n",
    "    path_saved_model = path_save_folder+\"/epoch_\" + str(epoch)\n",
    "    torch.save(lstm_model.state_dict(), path_saved_model)\n",
    "            \n",
    "    # print epoch report\n",
    "    report += f'| Epoch time: {\"%.1f \"% (time.time()-epoch_start_time)} s | LR:{\"%.5f \"% (optimizer.param_groups[0][\"lr\"])}'\n",
    "    print(report)\n",
    "    write_report(file_path=path_save_folder+\"/run_progress.txt\", text=report)\n",
    "    # modify learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "# print final report\n",
    "report = f'Total training time: {\"%.1f \"% (time.time()-training_time)} s'\n",
    "print(report)\n",
    "write_report(file_path=path_save_folder+\"/run_progress.txt\", text=report)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 5. Test LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case I already trained an LSTM I can re-construct the model\n",
    "lstm_model = CudaLSTM(hyperparameters=model_hyper_parameters).to(device)\n",
    "lstm_model.load_state_dict(torch.load(path_save_folder + \"/epoch_10\", map_location=device))\n",
    "\n",
    "# In case I already trained an LSTM I can re-construct the model\n",
    "lstm_model_inspect = customLSTM(hyperparameters=model_hyper_parameters).to(device)\n",
    "lstm_model_inspect.copy_weights(lstm_model)  # copy the CudaLSTM weights into the CustomLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.parameters of CudaLSTM(\n",
      "  (lstm): LSTM(26, 64, batch_first=True)\n",
      "  (dropout): Dropout(p=0.4, inplace=False)\n",
      "  (linear): Linear(in_features=64, out_features=1, bias=True)\n",
      ")>\n",
      "tensor([[ 0.2671, -0.0195,  0.0213,  ..., -0.0432,  0.0625,  0.0777],\n",
      "        [ 0.1592,  0.0181, -0.1605,  ...,  0.0888,  0.3064,  0.2707],\n",
      "        [ 0.0427,  0.0740, -0.1011,  ..., -0.3132,  0.1462,  0.2186],\n",
      "        ...,\n",
      "        [-0.0424,  0.1386,  0.0478,  ..., -0.2725, -0.3310, -0.6563],\n",
      "        [-0.1602,  0.0994,  0.4273,  ..., -0.5437, -0.2847,  0.0626],\n",
      "        [ 0.0360, -0.2027,  0.1956,  ..., -0.6539,  0.0249, -0.3405]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight',\n",
       "              tensor([[-2.2605e-01, -3.0040e-03,  1.3603e-01,  1.2626e-03,  8.3961e-03,\n",
       "                       -4.2549e-03,  6.7256e-02, -1.3690e-02,  1.3665e-02,  1.7328e-02,\n",
       "                        7.0537e-02,  1.1901e-02, -4.5101e-02, -1.0840e-02, -5.1693e-01,\n",
       "                        5.7178e-02, -1.0443e-02, -3.8384e-03, -9.0627e-03,  2.5756e-03,\n",
       "                       -5.1453e-03,  4.1203e-04, -1.2707e-02,  2.9975e-02, -5.8055e-01,\n",
       "                        5.2569e-02, -6.4860e-03, -1.8111e-03, -2.9278e-02,  5.3621e-02,\n",
       "                       -1.7695e-03, -5.8663e-01,  6.7572e-01,  2.3780e-02,  1.0828e+00,\n",
       "                       -5.2126e-02, -2.8526e-01, -4.6196e-02,  2.8321e-03, -4.2668e-02,\n",
       "                        4.4344e-03, -7.7555e-03, -1.4089e+00, -1.3269e-02,  6.4449e-01,\n",
       "                       -1.2535e-02, -6.2857e-02,  2.2457e-01, -8.9300e-03, -9.7877e-04,\n",
       "                        1.4886e-03, -9.6820e-02,  1.0922e-01,  1.5342e+00, -4.5128e-03,\n",
       "                        6.6947e-03,  2.7240e-02,  1.1678e+00, -3.4196e-03,  7.9135e-03,\n",
       "                        1.3966e+00, -7.4360e-02, -2.4785e-02, -2.1381e-02]])),\n",
       "             ('bias', tensor([-0.2018]))])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(lstm_model.parameters)\n",
    "print(getattr(lstm_model.lstm, \"weight_hh_l0\").data)\n",
    "lstm_model.linear.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.parameters of customLSTM(\n",
      "  (cell): _LSTMCell()\n",
      "  (dropout): Dropout(p=0.4, inplace=False)\n",
      "  (linear): Linear(in_features=64, out_features=1, bias=True)\n",
      ")>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight',\n",
       "              tensor([[-2.2605e-01, -3.0040e-03,  1.3603e-01,  1.2626e-03,  8.3961e-03,\n",
       "                       -4.2549e-03,  6.7256e-02, -1.3690e-02,  1.3665e-02,  1.7328e-02,\n",
       "                        7.0537e-02,  1.1901e-02, -4.5101e-02, -1.0840e-02, -5.1693e-01,\n",
       "                        5.7178e-02, -1.0443e-02, -3.8384e-03, -9.0627e-03,  2.5756e-03,\n",
       "                       -5.1453e-03,  4.1203e-04, -1.2707e-02,  2.9975e-02, -5.8055e-01,\n",
       "                        5.2569e-02, -6.4860e-03, -1.8111e-03, -2.9278e-02,  5.3621e-02,\n",
       "                       -1.7695e-03, -5.8663e-01,  6.7572e-01,  2.3780e-02,  1.0828e+00,\n",
       "                       -5.2126e-02, -2.8526e-01, -4.6196e-02,  2.8321e-03, -4.2668e-02,\n",
       "                        4.4344e-03, -7.7555e-03, -1.4089e+00, -1.3269e-02,  6.4449e-01,\n",
       "                       -1.2535e-02, -6.2857e-02,  2.2457e-01, -8.9300e-03, -9.7877e-04,\n",
       "                        1.4886e-03, -9.6820e-02,  1.0922e-01,  1.5342e+00, -4.5128e-03,\n",
       "                        6.6947e-03,  2.7240e-02,  1.1678e+00, -3.4196e-03,  7.9135e-03,\n",
       "                        1.3966e+00, -7.4360e-02, -2.4785e-02, -2.1381e-02]])),\n",
       "             ('bias', tensor([-0.2018]))])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(lstm_model_inspect.parameters)\n",
    "# print(getattr(lstm_model_inspect.lstm, \"weight_hh_l0\").data)\n",
    "lstm_model_inspect.linear.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will create an individual dataset per basin. This will give us more flexibility\n",
    "#path_entities = \"../../data/basin_id/basins_camels_gb_lees.txt\"\n",
    "entities_ids = np.loadtxt(path_entities, dtype=\"str\").tolist()[20:21]\n",
    "testing_dataset = {}\n",
    "\n",
    "# We can read a previously stored scaler or use the one from the training dataset we just generated\n",
    "#scaler = training_dataset.scaler\n",
    "with open(path_save_folder + \"/scaler.pickle\", \"rb\") as file:\n",
    "    scaler = pickle.load(file)\n",
    "\n",
    "for entity in entities_ids:\n",
    "    dataset = CAMELS_CH(dynamic_input= dynamic_input,\n",
    "                        target= target, \n",
    "                        sequence_length= model_hyper_parameters[\"seq_length\"],\n",
    "                        time_period= testing_period,\n",
    "                        path_data= path_data,\n",
    "                        entity= entity,\n",
    "                        static_input= static_input,\n",
    "                        check_NaN= False)\n",
    "    \n",
    "    dataset.scaler = scaler\n",
    "    dataset.standardize_data(standardize_output=False)\n",
    "    testing_dataset[entity]= dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing CudaLSTM\n",
    "\n",
    "lstm_model.eval()\n",
    "test_results = {}\n",
    "with torch.no_grad():\n",
    "    for basin, dataset in testing_dataset.items():\n",
    "        loader = DataLoader(dataset = dataset, \n",
    "                            batch_size = model_hyper_parameters[\"batch_size_evaluation\"], \n",
    "                            shuffle = False, \n",
    "                            drop_last = False) \n",
    "        df_ts = pd.DataFrame()\n",
    "        for sample in loader:\n",
    "            pred  = lstm_model(sample[\"x_lstm\"].to(device)) \n",
    "            # backtransformed information\n",
    "            y_sim = pred[\"y_hat\"]* dataset.scaler[\"y_std\"].to(device) + dataset.scaler[\"y_mean\"].to(device)\n",
    "            \n",
    "            # join results in a dataframe and store them in a dictionary (is easier to plot later)\n",
    "            df = pd.DataFrame({\"y_obs\": sample[\"y_obs\"].flatten().cpu().detach(),\n",
    "                               \"y_hat\": pred[\"y_hat\"].flatten().cpu().detach(),\n",
    "                                \"y_sim\": y_sim.flatten().cpu().detach()}, \n",
    "                                index=pd.to_datetime(sample[\"date\"]))\n",
    "\n",
    "            df_ts = pd.concat([df_ts, df], axis=0)\n",
    "\n",
    "            # remove from cuda\n",
    "            del pred, y_sim\n",
    "            torch.cuda.empty_cache()       \n",
    "        \n",
    "        test_results[basin] = df_ts\n",
    "\n",
    "# Save results as a pickle file\n",
    "with open(path_save_folder+\"/test_results_select.pickle\", \"wb\") as f:\n",
    "   pickle.dump(test_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1826, 365, 26])\n"
     ]
    }
   ],
   "source": [
    "# testing customLSTM\n",
    "\n",
    "lstm_model_inspect.eval()\n",
    "inspect_results = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for basin, dataset in testing_dataset.items():\n",
    "        loader = DataLoader(dataset = dataset, \n",
    "                            batch_size = model_hyper_parameters[\"batch_size_evaluation\"], \n",
    "                            shuffle = False, \n",
    "                            drop_last = False)\n",
    "\n",
    "        inspectmodel_output = []\n",
    "        df_ts = pd.DataFrame()\n",
    "\n",
    "        for sample in loader:\n",
    "            # using customLSTM\n",
    "            print(sample[\"x_lstm\"].shape)\n",
    "            pred_inspect  = lstm_model_inspect(sample[\"x_lstm\"].transpose(0, 1).to(device))\n",
    "            \n",
    "            y_sim = pred_inspect[\"y_hat\"]* dataset.scaler[\"y_std\"].to(device) + dataset.scaler[\"y_mean\"].to(device)\n",
    "\n",
    "            # join results in a dataframe and store them in a dictionary (is easier to plot later)\n",
    "            df = pd.DataFrame({\"y_obs\": sample[\"y_obs\"].flatten().cpu().detach(), \n",
    "                                \"y_sim\": y_sim.flatten().cpu().detach()}, \n",
    "                                index=pd.to_datetime(sample[\"date\"]))\n",
    "\n",
    "            df_ts = pd.concat([df_ts, df], axis=0)\n",
    "\n",
    "            _output = {\"h_n\": pred_inspect[\"h_n\"].cpu().detach(), \n",
    "                                \"c_n\": pred_inspect[\"c_n\"].cpu().detach(),\n",
    "                                \"i\":pred_inspect[\"i\"].cpu().detach(),\n",
    "                                \"f\":pred_inspect[\"f\"].cpu().detach(),\n",
    "                                \"g\":pred_inspect[\"g\"].cpu().detach(),\n",
    "                                \"o\":pred_inspect[\"o\"].cpu().detach(),\n",
    "                                \"y_hat\":y_sim.cpu().detach()}\n",
    "\n",
    "            inspectmodel_output.append(_output)\n",
    "\n",
    "            del pred_inspect\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        inspect_results[basin] = inspectmodel_output\n",
    "        del inspectmodel_output\n",
    "        print('Testing using CustomLSTM complete for ' + str(basin))\n",
    "\n",
    "# Save results as a pickle file\n",
    "with open(path_save_folder+\"/test_results_inspection.pickle\", \"wb\") as f:\n",
    "   pickle.dump(inspect_results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Part 6. Initial analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case we already ran our model\n",
    "with open(path_save_folder+\"/test_results.pickle\", \"rb\") as f:\n",
    "   test_results = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_save_folder+\"/test_results_inspection.pickle\", \"rb\") as f:\n",
    "   inspect_results = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loss testing\n",
    "loss_testing = nse(df_results=test_results, average=False)\n",
    "df_NSE = pd.DataFrame(data={\"basin_id\": testing_dataset.keys(), \"NSE\": np.round(loss_testing,3)})\n",
    "df_NSE = df_NSE.set_index(\"basin_id\")\n",
    "df_NSE.to_csv(path_save_folder+\"/NSE.csv\", index=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the histogram\n",
    "plt.hist(df_NSE[\"NSE\"], bins=[0, .1, .2, .3, .4, .5, .6, .7, .8, .9, 1])\n",
    "\n",
    "# Add NSE statistics to the plot\n",
    "plt.text(0.01, 0.8, f'Mean: {\"%.3f\" % df_NSE[\"NSE\"].mean():>7}\\nMedian: {\"%.3f\" % df_NSE[\"NSE\"].median():>0}\\nMax: {\"%.2f\" % df_NSE[\"NSE\"].max():>9}\\nMin: {\"%.2f\" % df_NSE[\"NSE\"].min():>10}',\n",
    "         transform=plt.gca().transAxes, bbox=dict(facecolor=\"white\", alpha=0.5))\n",
    "\n",
    "# Format plot\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 5)\n",
    "plt.xlabel(\"NSE\", fontsize=12, fontweight=\"bold\")\n",
    "plt.ylabel(\"Frequency\", fontsize=12, fontweight=\"bold\")\n",
    "plt.title(\"NSE Histogram\", fontsize=16, fontweight=\"bold\")\n",
    "#plt.savefig(save_folder+\"/NSE_LSTM_Histogram.png\", bbox_inches=\"tight\", pad_inches=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot simulated and observed discharges\n",
    "basin_to_analyze = \"2386\"\n",
    "\n",
    "plt.plot(test_results[basin_to_analyze][\"y_obs\"], label=\"observed\", color=color_palette[\"observed\"])\n",
    "plt.plot(test_results[basin_to_analyze][\"y_sim\"], label=\"simulated\", alpha=0.5, color=color_palette[\"simulated\"])\n",
    "\n",
    "# Format plot\n",
    "plt.xlabel(\"Day\", fontsize=12, fontweight=\"bold\")\n",
    "plt.ylabel(\"Discharge [mm/d]\", fontsize=12, fontweight=\"bold\")\n",
    "plt.title(\"Discharge comparison\", fontsize=16, fontweight=\"bold\")\n",
    "plt.tick_params(axis=\"both\", which=\"major\", labelsize=12)\n",
    "plt.legend(loc=\"upper right\",fontsize=12)\n",
    "plt.show()\n",
    "#plt.savefig(save_folder+\"/Model_Comparison.png\", bbox_inches=\"tight\", pad_inches=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 7: Inspecting LSTM activations and states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check if predictions of CustomLSTM and CudaLSTM are identical\n",
    "\n",
    "model_output = test_results[basin_to_analyze]\n",
    "inspectmodel_output = inspect_results[basin_to_analyze]\n",
    "\n",
    "print('Identical predictions:', torch.allclose(torch.tensor(model_output['y_sim']), inspectmodel_output[0]['y_hat'], atol=1e-5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_output['y_sim'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print((inspectmodel_output[0]['y_hat']-scaler[\"y_mean\"])/scaler[\"y_std\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(model_output['y_sim'])\n",
    "plt.plot(inspectmodel_output[0]['y_hat'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Concatenate all batches into one tensor that contains the final time step of each sample.\n",
    "# basin to analyse = 2386\n",
    "\n",
    "inspectmodel_output = inspect_results[basin_to_analyze]\n",
    "cell_states = torch.cat([out['c_n'][:, -1, :] for out in inspectmodel_output], dim=0).numpy()\n",
    "\n",
    "# Load the forcings input for the corresponding date range\n",
    "date_range = pd.date_range(testing_period[0], testing_period[1], freq='1D')\n",
    "\n",
    "# loading input data\n",
    "from pathlib import Path\n",
    "path_timeseries = Path(path_data) / 'timeseries' / 'observation_based' / f'CAMELS_CH_obs_based_{basin_to_analyze}.csv'\n",
    "# load time series\n",
    "df = pd.read_csv(path_timeseries)\n",
    "df = df.set_index('date')\n",
    "df.index = pd.to_datetime(df.index, format=\"%Y-%m-%d\")\n",
    "swe = df['swe(mm)'][date_range]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax, ax2) = plt.subplots(2, 1, figsize=(15, 6), sharex=True)\n",
    "\n",
    "ax2.plot(date_range, swe)\n",
    "ax.plot(date_range, cell_states, alpha=.2)\n",
    "\n",
    "ax.set_ylabel('cell state')\n",
    "ax2.set_ylabel('snow water equivalent')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspectmodel_output[0]['i'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(4, 2, figsize=(20, 14), sharex=True)\n",
    "ax[0,0].set_title('Input values')\n",
    "lines = ax[0,0].plot(swe)  # observed swe values\n",
    "# ax[0,0].legend(lines, cudalstm_config.dynamic_inputs, frameon=False)\n",
    "\n",
    "inspectmodel_output = inspect_results[basin_to_analyze]\n",
    "ax[1,0].set_title('Cell state')\n",
    "ax[1,0].plot(date_range,inspectmodel_output[0]['c_n'][0, :, 5].numpy())\n",
    "\n",
    "ax[0,1].set_title('Hidden state')\n",
    "ax[0,1].plot(date_range,inspectmodel_output[0]['h_n'][0].numpy())\n",
    "\n",
    "ax[1,1].set_title('Output gate')\n",
    "ax[1,1].plot(date_range,inspectmodel_output[0]['o'][0].numpy())\n",
    "\n",
    "ax[2,0].set_title('Forget gate')\n",
    "ax[2,0].plot(date_range,inspectmodel_output[0]['f'][0].numpy())\n",
    "\n",
    "ax[2,1].set_title('Input gate')\n",
    "ax[2,1].plot(date_range,inspectmodel_output[0]['i'][0].numpy())\n",
    "\n",
    "ax[3,0].set_title('Cell input activation')\n",
    "ax[3,0].plot(date_range,inspectmodel_output[0]['g'][0].numpy())\n",
    "\n",
    "f.delaxes(ax[3,1])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.2-0.m111",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.2-0:m111"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "42b7dc197ee81dd2f6541889b0e14556b882d218c1e7c97db94bc0f7b191f034"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
